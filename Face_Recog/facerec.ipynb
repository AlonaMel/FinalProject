{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This file is a detailed version including creating and training model, if you want to run the file and see the program working directly, please see the file \"facerec_direct.ipynb\" to run it by only click run all.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**step 1: get photos(no need to run this part)**\n",
    "\n",
    "**we use getTrainingData function below to get training photos for all of us four in our group and store these pictures in corresponding folders.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingData(window_name, camera_id, path_name, max_num): # path_name the path you store your photos，max_num is the number of photos you need \n",
    "    cv2.namedWindow(window_name) # create window\n",
    "    cap = cv2.VideoCapture(camera_id) # open camera\n",
    "    classifier = cv2.CascadeClassifier('haarcascade_frontalface_alt2.xml') # load classifier\n",
    "    color = (0,255,0) \n",
    "    num = 0 # count the pictures\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faceRects=classifier.detectMultiScale(gray,scaleFactor=1.2,minNeighbors=3,minSize=(32,32))\n",
    "        \n",
    "        if len(faceRects) > 0:\n",
    "            for faceRect in faceRects:\n",
    "                x,y,w,h = faceRect\n",
    "                image_name = '%s%d.jpg' % (path_name, num)\n",
    "                image = frame[y:y+h, x:x+w]\n",
    "                cv2.imwrite(image_name, image)\n",
    "                \n",
    "                num += 1\n",
    "                if num > max_num:\n",
    "                    break\n",
    "                \n",
    "                cv2.rectangle(frame, (x,y), (x+w,y+h), color, 2)\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                cv2.putText(frame, ('%d'%num), (x+30, y+30), font, 1, (255,0,255), 4)\n",
    "        if num > max_num:\n",
    "            break\n",
    "        cv2.imshow(window_name, frame)\n",
    "        c = cv2.waitKey(10)\n",
    "        if c & 0xFF == ord('q'):# press q to quit\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catching your face and writting into disk...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "print ('catching your face and writting into disk...')\n",
    "getTrainingData('getTrainData',0,'training_data_me/',600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**step 2:(dont need to run this, training_data_others is already prepared by us)**\n",
    "\n",
    "\n",
    "**prepare training_data_others. we use the photos of people whose name starts with letter A of LFW dataset as the data of others training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "num = 0\n",
    "finished = False\n",
    "def read_lfw(lfw_path):\n",
    "    global num, finished\n",
    "    for dir_item in os.listdir(lfw_path):\n",
    "        full_path = os.path.abspath(os.path.join(lfw_path, dir_item))\n",
    "        \n",
    "        if os.path.isdir(full_path): # if it is folder, recursively read the folder\n",
    "            read_lfw(full_path)\n",
    "        else: # if it is file\n",
    "            if dir_item.endswith('.jpg'):\n",
    "                image = cv2.imread(full_path)\n",
    "                classifier = cv2.CascadeClassifier('haarcascade_frontalface_alt2.xml') # load classifier\n",
    "                path_name = 'training_data_others/'\n",
    "                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                faceRects=classifier.detectMultiScale(gray,scaleFactor=1.2,minNeighbors=3,minSize=(32,32))\n",
    "        \n",
    "                if len(faceRects) > 0:\n",
    "                    for faceRect in faceRects:\n",
    "                        x,y,w,h = faceRect\n",
    "                        image_name = '%s%d.jpg' % (path_name, num) \n",
    "                        image = image[y:y+h, x:x+w] # store the face photo\n",
    "                        cv2.imwrite(image_name, image)\n",
    "                        num += 1\n",
    "                        if num > 3000:\n",
    "                            finished = True\n",
    "                            break\n",
    "        if finished:\n",
    "            print('Finished.')\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Processing lfw dataset...')\n",
    "read_lfw('lfw/') # lfw folder will affect step1, so we have already extracted photos and put them into training_data_others.So lfw is not in our project file now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**step 3: (you could run from here)**\n",
    "\n",
    "**we exctract the images and labels into numpy arrays and split those into training and testing dataset, after that we create and train CNN models which got validation accuracy higher than 99%. Then save the model in the model folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 128 # we assign the image as (128,128,3)\n",
    "\n",
    "def resize_image(image, height = IMAGE_SIZE, width = IMAGE_SIZE):\n",
    "    top, bottom, left, right = (0,0,0,0)\n",
    "    \n",
    "    h, w, _ = image.shape\n",
    "    \n",
    "    # find the longer edge if the image is not square\n",
    "    longest_edge = max(h,w)\n",
    "    \n",
    "    if h < longest_edge:\n",
    "        dh = longest_edge - h\n",
    "        top = dh // 2\n",
    "        bottom = dh - top\n",
    "    elif w < longest_edge:\n",
    "        dw = longest_edge - w\n",
    "        left = dw // 2\n",
    "        right = dw - left\n",
    "    else:\n",
    "        pass \n",
    "    \n",
    "    BLACK = [0,0,0]\n",
    "    constant = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value = BLACK)\n",
    "    return cv2.resize(constant, (height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "\n",
    "# path_name is working directory，we can get it by os.getcwd() later when we use this function\n",
    "def read_path(path_name):\n",
    "    for dir_item in os.listdir(path_name):\n",
    "        \n",
    "        full_path = os.path.abspath(os.path.join(path_name, dir_item))\n",
    "        \n",
    "        if os.path.isdir(full_path):\n",
    "            read_path(full_path)\n",
    "        else: #if it is fil\n",
    "            if dir_item.endswith('.jpg'):\n",
    "                image = cv2.imread(full_path)\n",
    "                if image is None:\n",
    "                    pass\n",
    "                else:\n",
    "                    image = resize_image(image, IMAGE_SIZE, IMAGE_SIZE)\n",
    "                    images.append(image)\n",
    "                    labels.append(path_name)\n",
    "    return images, labels\n",
    "\n",
    "def load_dataset(path_name):\n",
    "    images,labels = read_path(path_name)\n",
    "    images = np.array(images, dtype='float')\n",
    "    dic = {}\n",
    "    dic['me'] = 0   #me\n",
    "    dic['ei'] = 1   #yifei\n",
    "    dic['ng'] = 2   #liang\n",
    "    dic['rs'] = 3   #others\n",
    "    dic['ow'] = 4   #yellow\n",
    "    labels = np.array([dic[i[-2:]] for i in labels])\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, path_name): \n",
    "        # train\n",
    "        self.train_images = None\n",
    "        self.train_labels = None\n",
    "        \n",
    "        # test\n",
    "        self.test_images = None\n",
    "        self.test_labels = None\n",
    "        \n",
    "        self.path_name = path_name\n",
    "        \n",
    "        self.input_shape = None \n",
    "    \n",
    "    # load dataset, cross validation\n",
    "    def load(self, img_rows = IMAGE_SIZE, img_cols = IMAGE_SIZE, img_channels = 3, nb_classes = 5): #5 different classes\n",
    "        images, labels = load_dataset(self.path_name)\n",
    "        \n",
    "        train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size = 0.3, random_state = random.randint(0, 100))\n",
    "\n",
    "       # use tensorflow as backend\n",
    "        if K.image_data_format == 'channel_first':\n",
    "            train_images = train_images.reshape(train_images.shape[0],img_channels, img_rows, img_cols)\n",
    "            test_images = test_images.reshape(test_images.shape[0],img_channels, img_rows, img_cols)\n",
    "            self.input_shape = (img_channels, img_rows, img_cols)\n",
    "        else:\n",
    "            train_images = train_images.reshape(train_images.shape[0], img_rows, img_cols, img_channels)\n",
    "            test_images = test_images.reshape(test_images.shape[0], img_rows, img_cols, img_channels)\n",
    "            self.input_shape = (img_rows, img_cols, img_channels)\n",
    "\n",
    "        print(train_images.shape[0], 'train samples')\n",
    "        print(test_images.shape[0], 'test samples')\n",
    "        \n",
    "        train_labels = keras.utils.to_categorical(train_labels, nb_classes)\n",
    "        test_labels = keras.utils.to_categorical(test_labels,nb_classes)\n",
    "\n",
    "        # set pixels to 0~1\n",
    "        train_images /= 255\n",
    "        test_images /= 255\n",
    "        \n",
    "        self.train_images = train_images\n",
    "        self.test_images  = test_images\n",
    "        self.train_labels = train_labels\n",
    "        self.test_labels  = test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN model\n",
    "class Model:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        \n",
    "    def build_model(self, dataset, nb_classes = 5):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv2D(32, (3, 3), padding = 'same', input_shape = dataset.input_shape))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Conv2D(32, (3, 3)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "        self.model.add(Conv2D(64, (3, 3), padding = 'same'))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Conv2D(64, (3, 3)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "        self.model.add(Dropout(0.25))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(512))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dropout(0.25))\n",
    "        self.model.add(Dense(nb_classes))\n",
    "        self.model.add(Activation('softmax'))\n",
    "    def train(self, dataset, batch_size = 128, nb_epoch = 6):\n",
    "        \n",
    "        self.model.compile(loss = 'categorical_crossentropy', \n",
    "                           optimizer = 'ADAM',\n",
    "                           metrics = ['accuracy'])\n",
    "        self.model.fit(dataset.train_images, \n",
    "                           dataset.train_labels, \n",
    "                           batch_size = batch_size,\n",
    "                           epochs = nb_epoch, \n",
    "                           shuffle = True)\n",
    "\n",
    "    def evaluate(self, dataset):\n",
    "        score = self.model.evaluate(dataset.test_images, dataset.test_labels)\n",
    "        print(\"%s: %.3f%%\" % (self.model.metrics_names[1], score[1] * 100))\n",
    "        \n",
    "    def save_model(self, file_path):\n",
    "        self.model.save(file_path)\n",
    "    def load_model(self, file_path):\n",
    "        self.model = load_model(file_path)\n",
    "    def face_predict(self, image):\n",
    "        #use resize function defined before\n",
    "        image = resize_image(image)\n",
    "        image = image.reshape((1, IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "        image = image.astype('float32')\n",
    "        image /= 255\n",
    "        result = self.model.predict(image)\n",
    "        return result.argmax(axis=-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2126 train samples\n",
      "912 test samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<keras.layers.convolutional.Conv2D at 0x1a69f6d198>,\n",
       " <keras.layers.core.Activation at 0x1a69f6d6d8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1a69f6d240>,\n",
       " <keras.layers.core.Activation at 0x11fce0160>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x11fce0748>,\n",
       " <keras.layers.convolutional.Conv2D at 0x11fce0630>,\n",
       " <keras.layers.core.Activation at 0x1a69f67748>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1a80bd27b8>,\n",
       " <keras.layers.core.Activation at 0x11fce65c0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x11fce6c18>,\n",
       " <keras.layers.core.Dropout at 0x11fce6e48>,\n",
       " <keras.layers.core.Flatten at 0x1a5e07cc88>,\n",
       " <keras.layers.core.Dense at 0x1a5e07ccc0>,\n",
       " <keras.layers.core.Activation at 0x1a5e0b2da0>,\n",
       " <keras.layers.core.Dropout at 0x1a5e0b2d30>,\n",
       " <keras.layers.core.Dense at 0x1a5e0b2d68>,\n",
       " <keras.layers.core.Activation at 0x1a6a11b668>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "path_name = os.getcwd()  #get the current working directory\n",
    "dataset = Dataset(path_name)\n",
    "dataset.load(128,128,3,5)\n",
    "m = Model()\n",
    "m.build_model(dataset,5) #we have five classes\n",
    "m.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "2126/2126 [==============================] - 245s 115ms/step - loss: 1.4638 - acc: 0.5541\n",
      "Epoch 2/6\n",
      "2126/2126 [==============================] - 228s 107ms/step - loss: 0.1377 - acc: 0.9600\n",
      "Epoch 3/6\n",
      "2126/2126 [==============================] - 224s 106ms/step - loss: 0.0403 - acc: 0.9882\n",
      "Epoch 4/6\n",
      "2126/2126 [==============================] - 192s 90ms/step - loss: 0.0184 - acc: 0.9962\n",
      "Epoch 5/6\n",
      "2126/2126 [==============================] - 318s 149ms/step - loss: 0.0130 - acc: 0.9972\n",
      "Epoch 6/6\n",
      "2126/2126 [==============================] - 217s 102ms/step - loss: 0.0122 - acc: 0.9981\n"
     ]
    }
   ],
   "source": [
    "m.train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912/912 [==============================] - 34s 37ms/step\n",
      "acc: 99.561%\n"
     ]
    }
   ],
   "source": [
    "m.evaluate(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save_model('./model/facemodel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 4:\n",
    "The get_facecover method used to add a cover to the detected face. It required the left corner position (x,y), and the width and height(w,h) of target's face; the captured frame(img); the RGB image and alpha channel image (rgb_image, a) of the face mask.\n",
    "\n",
    "Firstly, The get_facecover method would resize the face mask based on the target's face size. Secondly, the method would use the alpha channel to get the face mask without white space. Thirdly, the method would add the face mask on the top of the target face which means it changed the captured frame and return the frame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Method\n",
    "def get_facecover(x, y, w, h, img, rgb_image, a):\n",
    "    eyes_center = ((2 * x + w)//2 , (2 * y)//2) # find the the center of the target's eyes\n",
    "#     factor = 1.2  # manual control size factor  \n",
    "    resized_image_h = h  # resized_image_h = h * factor\n",
    "    resized_image_w = w \n",
    "\n",
    "    if resized_image_h > y:\n",
    "        resized_image_h = y-1\n",
    "\n",
    "    # Resized image based on resized h and w\n",
    "    resized_image = cv2.resize(rgb_image,(resized_image_w,resized_image_h))\n",
    "    mask = cv2.resize(a,(resized_image_w,resized_image_h))\n",
    "    mask_inv =  cv2.bitwise_not(mask)\n",
    "    # Seted shifing offset\n",
    "    dh = int(1.015 * h)\n",
    "    dw = int(-0.2 * w) \n",
    "    \n",
    "    # Find face\n",
    "    bg_roi = img[y+dh-resized_image_h:y+dh,(eyes_center[0]-resized_image_w//3)+dw:(eyes_center[0]+resized_image_w//3*2)+dw]\n",
    "    bg_roi = bg_roi.astype(float)\n",
    "    \n",
    "    # Generate Mask\n",
    "    mask_inv = cv2.merge((mask_inv,mask_inv,mask_inv))\n",
    "    alpha = mask_inv.astype(float)/255\n",
    "\n",
    "    # resize alpha for future calculation\n",
    "    alpha = cv2.resize(alpha,(bg_roi.shape[1],bg_roi.shape[0]))\n",
    "    bg = cv2.multiply(alpha, bg_roi)\n",
    "    bg = bg.astype('uint8')\n",
    "\n",
    "\n",
    "    # Get the area of face mask\n",
    "    image = cv2.bitwise_and(resized_image,resized_image,mask = mask)\n",
    "    image = cv2.resize(image,(bg_roi.shape[1],bg_roi.shape[0]))\n",
    "    # Add face mask to the target area\n",
    "    add_image = cv2.add(bg,image)\n",
    "\n",
    "    # Replace the original with the face mask\n",
    "    img[y+dh-resized_image_h:y+dh,(eyes_center[0]-resized_image_w//3)+dw:(eyes_center[0]+resized_image_w//3*2)+dw] = add_image.copy()\n",
    "    \n",
    "    # Return the edited frame\n",
    "    return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images Pre-processing \n",
    "\n",
    "\n",
    "\n",
    "This step is used to get the face masks' data. Our model would detect 7 different emotions: angry, hate, fear, happy, sad, surprise and neutral which means that we need 7 face masks. We used OpenCV library to read and split each .png image in r, g, b, a channels. In next step, we merged the r,g and b channels to rgb_face. Then the rgb channels data and alpha channel data are saved in facial_dict:\n",
    "\n",
    "                facial_dict[emotion] = [rgb_channels, alpha_channel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "facial_file = ['angry','hate','fear','happy','sad','surprise','neutral']\n",
    "\n",
    "facial_dict = {}\n",
    "for i in range(7):\n",
    "    print(i)\n",
    "    face_img = cv2.imread(facial_file[i] + '.png',cv2.IMREAD_UNCHANGED)\n",
    "    r,g,b,a = cv2.split(face_img) \n",
    "    rgb_face = cv2.merge((r,g,b))\n",
    "    cv2.imwrite(facial_file[i]+'_alpha.png',a)\n",
    "    facial_dict[facial_file[i]] = [rgb_face,a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parrow/.local/lib/python3.6/site-packages/keras/engine/saving.py:327: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import sys\n",
    "from keras.models import load_model\n",
    "# Def emotions labels\n",
    "emotion_labels = {\n",
    "    0: 'angry',\n",
    "    1: 'hate',\n",
    "    2: 'fear',\n",
    "    3: 'happy',\n",
    "    4: 'sad',\n",
    "    5: 'surprise',\n",
    "    6: 'neutral'\n",
    "}\n",
    "\n",
    "# Load emotion classifier\n",
    "emotion_classifier = load_model('model/simple_CNN.530-0.65.hdf5')\n",
    "# Load face detection model\n",
    "model = Model()\n",
    "model.load_model(file_path = './model/facemodel.h5')    \n",
    "\n",
    "# Create OpenCv window\n",
    "cv2.namedWindow('Detecting your face.') \n",
    "color = (0, 255, 0)\n",
    "\n",
    "# Load haarcascade classifier\n",
    "classifier = cv2.CascadeClassifier('haarcascade_frontalface_alt2.xml')\n",
    "# Capture the real time image frame\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "        ok, frame = cap.read() # type(frame) <class 'numpy.ndarray'>\n",
    "        if not ok:\n",
    "            break\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # gray scale the frame\n",
    "        frame2 = frame.copy()\n",
    "        faceRects=classifier.detectMultiScale(gray,scaleFactor=1.2,minNeighbors=3,minSize=(32,32))\n",
    "        \n",
    "        # If detected target face\n",
    "        if len(faceRects) > 0:\n",
    "            \n",
    "            for faceRect in faceRects: \n",
    "                \n",
    "                x, y, w, h = faceRect                \n",
    "                image = frame[y - 10: y + h + 10, x - 10: x + w + 10].copy()\n",
    "                \n",
    "                \n",
    "                # Dectect if the image is none\n",
    "                # if we won't do this step, there will be an error:\n",
    "                    # error error (-215) ssize.width > 0 && ssize.height > 0 in function cv::resize\n",
    "                if image is None:  \n",
    "                    break\n",
    "                else:\n",
    "                    if image is not None:\n",
    "                        gray_face = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                        gray_face = cv2.resize(gray_face,(48,48))\n",
    "                        gray_face = gray_face/255.0\n",
    "                        gray_face =np.expand_dims(gray_face,0)\n",
    "                        gray_face =np.expand_dims(gray_face,-1)\n",
    "                        \n",
    "                        # Predict emotion from the gray scale image\n",
    "                        emotion_label_arg = np.argmax(emotion_classifier.predict(gray_face))\n",
    "                        emotion = emotion_labels[emotion_label_arg]\n",
    "                        faceID = model.face_predict(image)\n",
    "                        \n",
    "                        # Make a face mask based on the emotions\n",
    "                        img = frame2.copy()\n",
    "                        img = get_facecover(x, y, w, h, img, facial_dict[emotion][0], facial_dict[emotion][1])\n",
    "                        frame2 = img.copy()\n",
    "                        \n",
    "                        # Show the target's name and emotion\n",
    "                        if faceID[0] == 0:                                                        \n",
    "                            cv2.rectangle(frame2, (x - 10, y - 10), (x + w + 10, y + h + 10), color, thickness = 2)\n",
    "                            cv2.putText(frame2,'Yan is '+ emotion, \n",
    "                                    (x + 30, y + 30),                      # Coordinate\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX,              # Font\n",
    "                                    1,                                     # Word size\n",
    "                                    (255,0,255),                           # Word color\n",
    "                                    2)                                     # Word's line width\n",
    "                            \n",
    "                        elif faceID[0] == 1:\n",
    "                            cv2.rectangle(frame2, (x - 10, y - 10), (x + w + 10, y + h + 10), color, thickness = 2)\n",
    "                            cv2.putText(frame2,'Yujie is ' + emotion, \n",
    "                                    (x + 30, y + 30),                     \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX,             \n",
    "                                    1,                                     \n",
    "                                    (255,0,255),                        \n",
    "                                    2)                                   \n",
    "                        elif faceID[0] == 2:\n",
    "                            cv2.rectangle(frame2, (x - 10, y - 10), (x + w + 10, y + h + 10), color, thickness = 2)\n",
    "                            cv2.putText(frame2,'Liang is ' + emotion, \n",
    "                                    (x + 30, y + 30),                      \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX,             \n",
    "                                    1,                                  \n",
    "                                    (255,0,255),                          \n",
    "                                    2)                                     \n",
    "                        elif faceID[0] == 4:\n",
    "                            cv2.rectangle(frame2, (x - 10, y - 10), (x + w + 10, y + h + 10), color, thickness = 2)\n",
    "                            cv2.putText(frame2,'Zuxian is ' + emotion, \n",
    "                                    (x + 30, y + 30),                     \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX,             \n",
    "                                    1,                                  \n",
    "                                    (255,0,255),                           \n",
    "                                    2)                                     \n",
    "                        else:\n",
    "                            cv2.rectangle(frame2, (x - 10, y - 10), (x + w + 10, y + h + 10), color, thickness = 2)\n",
    "                            cv2.putText(frame2,'Unknown is '+ emotion, \n",
    "                                    (x + 30, y + 30),                    \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX,            \n",
    "                                    1,                                   \n",
    "                                    (255,0,255),                         \n",
    "                                    2)    \n",
    "        # Show the edited frame on the screen\n",
    "                \n",
    "        cv2.imshow(\"Detecting your face.\", frame2)\n",
    "        \n",
    "        # Press to q to exit\n",
    "       \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "# Release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
